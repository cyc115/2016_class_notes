#TOC
[TOC]
#Lecture 23 : Markov Decision Processes 

##Markov Chain:

- set of state `s`
- transition probability : `T: S X S -> [0,1]`
	- $$T(s,s') = P(S_{t+1} = S' | S_t = S) $$ 
- initial state distribution 
	- $$P_0 : S \rightarrow [0,1]$$
	- $$P_0(s) = P(S_0) $$

##decision making with Markov Chain

- states do depend on previous state. 
- `actions` and `rewards` and `state` are distinctive
	- ![enter image description here](https://lh3.googleusercontent.com/-5s4o9q97Jp8/Vw7MFLFd4PI/AAAAAAAAC0U/RNsL581ceVkI3icJwdtuS1EXhpL1nCY4ACKgB/s0/Selection_171.jpg "Selection_171.jpg")
	- action : circle 
	- reward : dimond 
	- Action : square 

##Markov Decision Process (MDP)
- set of states $S$
- set of actions $A$
- Transition model : $T: S * A * S \in [0,1]$
- Reward Function : $R: S * A \rightarrow R $ (short term utility function ) 
- Discount Factor $\gamma \in [0,1]$ 

##Planning in MDP 
- **maximizing expected utility** (long term) 
- maximize immediate utility is not sufficient

##example : Mountain - car 

**States** : position and velocity 
**Actions** : accelerated forward, backward , const
**Goal** : get car to the top of the hill as quickly as possible 
**Reward** : -1 for every time spent. alternative : R = 1 at the top, 0 otherwise. 

##policies
###two types 
- deterministic : in each state agent chooses a unique action as good option.  **Optimal**
- stochastic : in the same state , the agent can choose different actions. 

once policy is fixed the mdp becomes a markov chain with reward.

we want to find a policy to maximize the utility. So we want to estimate the expected utility. 

##Value function V(s)
represent the expected utility for every state, given a certain policy. 

value function of a policy $\pi$ is $$V^{\pi} : S \rightarrow R$$

value of state s under policy $\bf \pi$ : 
$$V^{\pi}(s) = E_{\pi} [U_t| S_t = s] $$

> ## Bellman equation for evaluating policy 
> $E_\pi $: expectation of policy $\pi$ 
> $U_t$ : is the utility function 
> $\gamma$ is the discount rate
> $$U_t = R_t + \gamma R_{t+1} + \gamma R_{t+2}  + \gamma R_{t+3} + ... $$  
> $$ = R_t + \gamma (R_{t+1} +  R_{t+2}  +  R_{t+3} + ... )$$  
> $$\bf{ = R_t + \gamma U_{t+1} }$$
> Value function now becomes 
> $$V^{\pi}(s) = E_{\pi} [U_t| S_t = s]  = E_{\pi} [R_t + \gamma U_{t+1}| S_t = s]  $$ 

> ##Bellman equation in matrix form 
> $$V^\pi  = R ^\pi + \gamma T^\pi V^\pi$$
> $V^\pi$ : a vector containing the value of each state under policy $\pi$
> $R^\pi $ : a vector containing immediate reward at each state $R(s, \pi(s))$
> $T^\pi $ : a matrix containing the transition probability at each state: $T(s, \pi(s) , s')$

##iterative policy evaluation 
1. start with initial guess $V_0$ 
2. do for k iterations : 
	$V_{k+1}(s) \leftarrow (R(s, \pi(s))  + \gamma \sum_{s' \in S}{T(s,\pi(s) , s') V_k(s')})$
3. stop when the max change between iterations falls below $\epsilon$ 

$\epsilon = max | V_k - V_{k+1}|$

##policy improvement 
recall $$\bf{ V^{\pi}(s) \leftarrow \sum_{a \in A} \pi(s,a)  (R(s, a)  + \gamma \sum_{s' \in S}{T(s,a , s') V(s')}) }$$

suppose some action $a^*$ :

$$(R(s, a^*)  + \gamma \sum_{s' \in S}{T(s,a^* , s') V(s')}) \gt V^{\pi} (s)$$

then if we set $\pi(s,a^*) \leftarrow 1$, then the value of state s will increase. 

we can change policy $\pi$ to $\pi^*$ which is **greedy** wrt $V^\pi$. 

$$\pi^*(s) = argmax_{a \in A } (R (s,a) + \gamma \sum_{s' \in S} T(s,a,s') V^{\pi} (s'))$$

search is guaranteed to terminate.



#What you should know
• Definition of MDP framework.
• Differences/similarities between MDPs and other AI approaches
(e.g. general search, game playing, STRIPS planning).
•Basic MDP algorithms and their properties:
– Policy evaluation
– Policy iteration

##complexity 
there are two iteration methods :

- value iteration 
	- each iteration : $O(S^2A)$ down to $O(SA)$ if few successor states 
	- # iterations : $\frac{1} {1-\gamma}$
	- faster per iteration 
- policy iteration 
	- each iteration $O(S^3 + S^2 A) $ 
	- # iteratoins $|A|^{|S|} $ 
	- less iterations 

##Limitations of MDP :
1. finding opt policy is polynomial states -> does not work for large number of states, solution : 
	- we can try to use **linear** approximation function for the value function $$V_w(S) = \sum _m W_m \theta_m(S) $$ where $\theta_{1..m} $ are basis features about the state and $W_{1..m}$ is the weight 
	- train **weights** from data using 
	- use ANN to calculate **VI** , may not be stable (non linear) 
2. VI and PI assumes transition and rewards is known ( solution in lec 25) 
3. states is unobservable sometimes 
	- states may look the same 


#lec 25  Reinforcement Learning 

![enter image description here](https://lh3.googleusercontent.com/-TYr5CD1h6S4/VxLbcP8C3tI/AAAAAAAAC3s/wGJIGRkTXYg0brATQVYGCC1ubgsJRTOngCKgB/s0/Selection_173.jpg "Selection_173.jpg")

**Trajectories** : $<s_0 , a_0  , r_0 ,... , s_n , a_n , r_n>$

break the trajectories into tuples $<s_0 , a_0  , r_0> ,... , <s_n , a_n , r_n>$
ignore dependencies between tuples... each T are independent event. 

##Learning MDP with RL 

###model-based reinforcement learning, MBRL 

1. given a policy for acquiring data. ( random exploration) 
2. Observe transition pairs $<s,a,r,s>$
3. learn an **approximate model** : $R(s,a)$ , $T(s,a,s')$ 
	4. MLE to compute probabilities 
	5. **supervised learning ** for reward
6. pretend the approx is correct, use dynamic programming method (value / policy iteration) 

people are not very interested in MBRL. 

###Modelless RL

> ## monte carlo's method  ##
> - identify a model of the activity
> - define parameters ( mean , stdiv) for each factor in the model 
> - create random data according to these parameters 
> - simulate and analysis 

Agent interacts with environment in trials or episodes that terminates behaving according to some policy $\pi$ , generating several trajectories. 

compute $V^\pi (s) $ by **averaging the observed return** after s on the  
trajectories in which s was visited

$U_i$ : observed utility from state **s** for the trajectory **i**

$V_{n+1}(s) $ : estimate of the value from state **s** after observing **n+1** trajectories starting at **s**. 


![enter image description here](https://lh3.googleusercontent.com/-S3CZZQzry3c/VxLxTVM_III/AAAAAAAAC4c/55Wk9GSLZ_Q1SV2dHZ8BvUnet_GultC_ACKgB/s0/Selection_174.jpg "Selection_174.jpg")
$\text{A running average ! $\rightarrow$ a gradient}$ 

The **learning rate** version : 
$$V(s_{t+1}) \leftarrow V(s_t) + \alpha(U(s_t) - V(s_t))$$ 

##TD learning Algorithm in contrast with monte carlo 

> TD is used to compute values for a given poplcy $\pi$ 
 
> p is a Policy 
> a is an Action 
> s is a State 
> s' is the resulting state 
> r is the reward of the action at state  
> take(a,s) : (s', r) take action in state s returns the resulting state and the reward of the action 
> V(s) : value of state 
>  Td error : $ \delta \leftarrow  r + \gamma V(s') - V(s) $

	while(! quit) :
		pick start state s 
		for t in time_steps:
			Action a = p(s)
			(s' , r) = take(a,s)
			TD_error = r + (deprecation_factor * s'.value - s.value)
			s.value = s.value + learning_factor * TD_error


##Online Gradient descent TD 

	init weight vector of the function approximator w 
	pick a state s 
	for t in time_step :
		Action a = policy(s)
		(s',r) = take(a,s)
		td_err = r + depreciation_rate * s'.val - s.val 
		w = w + learning_rate * td_err * gradient(s.val)
		s = s'
		
##advantage of TD 
- no model of the environment is required 
- incremental learning
- less memory and peak computation required 
- Both TD and MC converge. 
- TD learns faster 


##RL Algorithms for control 
The **behaviour policy** ( used to collect data) need to balance between 
	- **explore** environment 
	- **exploit** existing knowledge, taking the action that currently seems best 

###$\text{Exploration }\epsilon \text{ greedy} $

**Theory** : to get the opt solution : agent need to try all actions for all states many times... 

**practice** : 
Pr picking random action : $\epsilon$ , Pr (best action) = $1 - \epsilon$
$\epsilon = 0.1 $ , decreases over time or the number of times the current state is visited.


#Lec 26 Conclusion 


